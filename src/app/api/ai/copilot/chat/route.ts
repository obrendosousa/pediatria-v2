import { NextResponse } from "next/server";
import { HumanMessage, AIMessage } from "@langchain/core/messages";
import { createClient } from "@/lib/supabase/server";
import { requireApprovedProfile } from "@/lib/auth/requireApprovedProfile";
import { getSupabaseAdminClient } from "@/lib/automation/adapters/supabaseAdmin";
import { claraGraph, type ClaraState } from "@/ai/clara/graph";

type HistoryItem = {
  role: "user" | "assistant";
  content: string;
};

// N√≥s do grafo da Clara que produzem a resposta final vis√≠vel para o usu√°rio
// CORRE√á√ÉO: Adicionado o "final_report_node" para que o relat√≥rio chegue √† tela
const RESPONSE_NODES = new Set(["simple_agent", "final_report_node"]);

export async function POST(request: Request) {
  try {
    const supabase = await createClient();
    await requireApprovedProfile(supabase, { allowedRoles: ["admin", "secretary"] });

    const body = (await request.json()) as {
      chatId?: unknown;
      message?: unknown;
      history?: unknown;
    };

    const chatId = typeof body.chatId === "number" ? body.chatId : Number(body.chatId);
    const message = typeof body.message === "string" ? body.message.trim() : "";
    const history = Array.isArray(body.history) ? (body.history as HistoryItem[]) : [];

    if (!chatId || isNaN(chatId)) {
      return NextResponse.json({ error: "Campo 'chatId' √© obrigat√≥rio." }, { status: 400 });
    }
    if (!message) {
      return NextResponse.json({ error: "Campo 'message' √© obrigat√≥rio." }, { status: 400 });
    }

    const adminSupabase = getSupabaseAdminClient();

    // Busca nome do paciente
    const { data: chatData } = await (adminSupabase as any)
      .from("chats")
      .select("id, contact_name")
      .eq("id", chatId)
      .maybeSingle();

    const patientName = chatData?.contact_name || "Paciente";

    // Janela deslizante: √∫ltimas 20 mensagens do paciente para contexto imediato
    const { data: messagesData } = await (adminSupabase as any)
      .from("chat_messages")
      .select("sender, message_text, message_type, created_at")
      .eq("chat_id", chatId)
      .order("created_at", { ascending: false })
      .limit(20);

    const chatHistory = messagesData
      ? (messagesData as any[])
        .reverse()
        .map((msg) => {
          const isClinic =
            msg.sender === "HUMAN_AGENT" || msg.sender === "AI_AGENT" || msg.sender === "me";
          const senderName = isClinic ? "Cl√≠nica" : patientName;
          const content =
            msg.message_text?.trim() || `[M√≠dia: ${msg.message_type || "desconhecido"}]`;
          const timeStr = new Date(msg.created_at).toLocaleTimeString("pt-BR", {
            hour: "2-digit",
            minute: "2-digit",
          });
          return `[${timeStr}] ${senderName}: ${content}`;
        })
        .join("\n")
      : "";

    // MENSAGEM CONTEXTUALIZADA CORRIGIDA: 
    // Libera a IA para gerar relat√≥rios globais mesmo estando na tela de um paciente
    const contextualMessage = `[MODO COPILOTO ‚Äî ASSISTENTE DA CL√çNICA]
Voc√™ √© a Clara, assistente de intelig√™ncia artificial da cl√≠nica. O usu√°rio atual (admin/secret√°ria) est√° visualizando a tela do chat do paciente: ${patientName} (chat_id: ${chatId}).

HIST√ìRICO RECENTE DESTE PACIENTE (√∫ltimas 20 mensagens):
------------------------------------------------------
${chatHistory || "Nenhuma mensagem dispon√≠vel ainda."}
------------------------------------------------------

PERGUNTA DO USU√ÅRIO: ${message}

INSTRU√á√ïES CR√çTICAS DE CONTEXTO:
1. FOCO LOCAL: Se a pergunta for sobre ESTE paciente espec√≠fico, use o hist√≥rico acima e ajude a entender/resolver o caso.
2. FOCO GLOBAL (RELAT√ìRIOS): Se a pergunta for GERAL sobre a cl√≠nica (ex: "quantos atendimentos tivemos", "gere um relat√≥rio", "qual a m√©dia de notas", "resumo do dia", "faturamento"), IGNORE o paciente atual. Voc√™ tem a permiss√£o e o DEVER de usar suas ferramentas (como query_database, generate_sql_report, get_aggregated_insights, etc.) para varrer TODO o banco de dados e entregar o relat√≥rio geral exato.
3. N√£o invente dados. Use sempre as ferramentas de banco de dados para responder perguntas globais com absoluta certeza.`;

    // Reconstr√≥i hist√≥rico do mini-chat como messages LangChain (sem tool calls anteriores)
    const priorMessages = history
      .filter((item) => item.content?.trim())
      .map((item) =>
        item.role === "assistant" ? new AIMessage(item.content) : new HumanMessage(item.content)
      );

    const inputs: ClaraState = {
      messages: [...priorMessages, new HumanMessage(contextualMessage)],
      chat_id: chatId,   // Chat ID do PACIENTE
      is_deep_research: false,
      is_planning_mode: false,
      research_brief: "",
      raw_notes: [],
      supervisor_messages: [],
      supervisor_iteration: 0,
      research_complete: false,
      current_user_role: "admin",
    };

    const encoder = new TextEncoder();

    const stream = new ReadableStream({
      async start(controller) {
        const enqueue = (payload: object) =>
          controller.enqueue(encoder.encode(JSON.stringify(payload) + "\n"));

        try {
          const events = claraGraph.streamEvents(inputs as any, {
            version: "v2",
            configurable: { thread_id: `copilot_${chatId}_${Date.now()}` },
            streamMode: "values"
          });

          // Flag para o fallback: se nenhum chunk de texto for emitido, usa on_chain_end
          let chunksEmitted = 0;

          for await (const event of events) {
            // --- STATUS: Progresso dos n√≥s do grafo ---
            if (event.event === "on_chain_start") {
              const node: string = event.name ?? "";
              let label: string | null = null;
              if (node === "router_and_planner_node") label = "üß† Analisando complexidade da pergunta...";
              else if (node === "executor_node") label = "üîç Executando etapa da pesquisa...";
              else if (node === "reporter_node") label = "‚úçÔ∏è Elaborando resposta final...";
              else if (node === "fetch_data") label = "Baixando mensagens do chat para an√°lise...";
              else if (node === "analyze_conversation") label = "IA pensando e extraindo gargalos...";
              else if (node === "save_to_db") label = "Salvando insights e hist√≥rico no banco...";
              else if (node === "final_report_node") label = "‚úçÔ∏è Escrevendo relat√≥rio final...";
              if (label) enqueue({ type: "ui_log", content: label });
            }

            // --- STATUS: Chamadas de ferramenta ---
            if (event.event === "on_tool_start") {
              const toolName: string = event.name ?? "";
              let label = toolName;
              if (toolName === "get_chat_cascade_history") label = "üìñ Lendo hist√≥rico do paciente...";
              else if (toolName === "search_knowledge_base") label = "üîé Consultando base de conhecimento...";
              else if (toolName === "read_brain_files") label = "üß† Lendo contexto da cl√≠nica...";
              else if (toolName === "manage_long_term_memory") label = "üí≠ Consultando mem√≥ria da Clara...";
              else if (toolName === "get_filtered_chats_list") label = "üìã Listando chats...";
              else if (toolName === "deep_research_chats_tool" || toolName === "deep_research_chats") label = "‚öôÔ∏è Pesquisa profunda (Map-Reduce)...";
              else if (toolName === "analisar_chat_especifico") label = "üîé Analisando atendimento detalhadamente...";
              else if (toolName === "gerar_relatorio_qualidade_chats") label = "üìä Consolidando relat√≥rios de qualidade...";
              else if (toolName === "generate_sql_report" || toolName === "query_database") label = "üìä Consultando banco de dados global...";
              else label = `‚öôÔ∏è Executando a√ß√£o: ${toolName}...`;
              enqueue({ type: "ui_log", content: label });
            }

            // --- TOKENS DE TEXTO (modo streaming ideal) ---
            // Filtra para N√ÉO mostrar tokens do router_and_planner_node (JSON interno)
            // e N√ÉO mostrar chunks de tool calls (content array ou vazio)
            if (event.event === "on_chat_model_stream") {
              const node: string = event.metadata?.langgraph_node ?? "";
              if (RESPONSE_NODES.has(node)) {
                const content = event.data?.chunk?.content;
                if (typeof content === "string" && content.length > 0) {
                  enqueue({ type: "chunk", content });
                  chunksEmitted++;
                }
              }
            }

            // --- FALLBACK: se on_chat_model_stream n√£o disparou, captura texto no on_chain_end ---
            // Ocorre quando o modelo interno usa invoke() sem streaming habilitado
            if (event.event === "on_chain_end" && chunksEmitted === 0) {
              const node: string = event.name ?? "";
              if (RESPONSE_NODES.has(node)) {
                const output = event.data?.output as any;
                // O output do n√≥ √© o estado retornado: { messages: [...] }
                const msgs = output?.messages;
                if (Array.isArray(msgs) && msgs.length > 0) {
                  const lastMsg = msgs[msgs.length - 1];
                  const text =
                    typeof lastMsg?.content === "string"
                      ? lastMsg.content
                      : Array.isArray(lastMsg?.content)
                        ? (lastMsg.content as any[]).map((c: any) => c.text ?? "").join("")
                        : "";
                  if (text.trim()) {
                    enqueue({ type: "chunk", content: text });
                    chunksEmitted++;
                  }
                }
              }
            }
          }

          controller.close();
        } catch (err) {
          const msg = err instanceof Error ? err.message : "Erro durante a execu√ß√£o.";
          console.error("[/api/ai/copilot/chat] erro no stream:", err);
          controller.enqueue(encoder.encode(JSON.stringify({ type: "error", content: msg }) + "\n"));
          controller.close();
        }
      },
    });

    return new Response(stream, {
      headers: {
        "Content-Type": "application/x-ndjson",
        "Cache-Control": "no-cache",
        Connection: "keep-alive",
      },
    });
  } catch (error) {
    console.error("[/api/ai/copilot/chat] erro setup:", error);
    const message = error instanceof Error ? error.message : "Erro inesperado.";
    return NextResponse.json({ error: message }, { status: 500 });
  }
}